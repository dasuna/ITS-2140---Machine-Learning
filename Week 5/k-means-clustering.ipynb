{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4a8de4",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Clustering Wikipedia Articles with K-Means\n",
    "\n",
    "This notebook demonstrates how to fetch text documents from Wikipedia, process it, and then use **K-Means** to discover topic clusters. The key steps are:\n",
    "\n",
    "**Setup:** Installing and importing the required libraries.\n",
    "\n",
    "**Data Collection:** Fetching articles from Wikipedia.\n",
    "\n",
    "**Text Preprocessing:** Cleaning the text data.\n",
    "\n",
    "**Vectorization:** Converting text into numerical TF-IDF vectors.\n",
    "\n",
    "**Clustering:** Applying the K-Means algorithm.\n",
    "\n",
    "**Analysis:** Inspecting the clusters to understand their topics.\n",
    "\n",
    "**Prediction:** Categorizing a new document using the trained model.\n",
    "\n",
    "You may need to install the libraries below.\n",
    "\n",
    "pip3 install wikipedia-api\n",
    "\n",
    "pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811bf041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data downloaded successfully using the manual method.\n",
      "Libraries imported and NLTK data downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dasunathukolage/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dasunathukolage/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/dasunathukolage/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Imports ---\n",
    "import wikipediaapi\n",
    "import nltk\n",
    "import ssl\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Now, try to download the data again\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "print(\"NLTK data downloaded successfully using the manual method.\")\n",
    "\n",
    "print(\"Libraries imported and NLTK data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ca0e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched: Galaxy\n",
      "Successfully fetched: Black hole\n",
      "Successfully fetched: Supernova\n",
      "Successfully fetched: DNA\n",
      "Successfully fetched: Photosynthesis\n",
      "Successfully fetched: Evolution\n",
      "Successfully fetched: Machine learning\n",
      "Successfully fetched: Artificial intelligence\n",
      "Successfully fetched: Computer programming\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Fetch Wikipedia Articles ---\n",
    "\n",
    "# List of articles to cluster. We've chosen topics in astronomy, biology, and computer science.\n",
    "article_titles = [\n",
    "    \"Galaxy\", \"Black hole\", \"Supernova\", # Astronomy\n",
    "    \"DNA\", \"Photosynthesis\", \"Evolution\", # Biology\n",
    "    \"Machine learning\", \"Artificial intelligence\", \"Computer programming\" # Computer Science\n",
    "]\n",
    "\n",
    "# Initialize the Wikipedia API\n",
    "wiki_api = wikipediaapi.Wikipedia('MyClusteringProject/1.0', 'en')\n",
    "\n",
    "documents = []\n",
    "for title in article_titles:\n",
    "    page = wiki_api.page(title)\n",
    "    if page.exists():\n",
    "        documents.append(page.text)\n",
    "        print(f\"Successfully fetched: {title}\")\n",
    "    else:\n",
    "        print(f\"Could not find page: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf37cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words:\n",
      " {'had', 'during', \"hasn't\", 'y', 'yourself', 'which', 'ma', 'against', 'won', 'the', \"you've\", 'now', \"you'll\", \"couldn't\", 'what', 'other', \"it'll\", 'we', 'below', 'ain', 'it', 'with', 'from', \"we'll\", 'aren', 'yourselves', 'not', 'myself', 'haven', 'should', 'am', 'mustn', 'if', 'he', 'them', 'she', 'up', \"won't\", 'whom', 'can', 'o', \"mightn't\", 'both', 'further', 'out', \"wasn't\", \"she'd\", 'did', 'how', 'do', 'into', 'on', 'ours', 'where', 'when', \"shan't\", 'you', 'shan', 'his', 'him', \"he'll\", 'hasn', 'itself', 'so', 'these', 'has', 'under', 'having', 'more', 'own', 'have', \"isn't\", \"i've\", 'their', \"aren't\", 'was', \"hadn't\", 'at', \"they've\", 'mightn', \"they're\", 'an', 'to', 'only', 'than', 'but', 'isn', 'about', 'nor', \"she'll\", 'doesn', 'this', 'off', 'me', 'i', 'no', 'just', 'didn', 'over', 'yours', 'doing', 'be', 'hers', 'before', 't', 'or', 'after', 'while', 'most', \"should've\", 'don', \"you'd\", \"she's\", \"mustn't\", 'wasn', \"that'll\", 'hadn', \"doesn't\", \"he's\", 'll', \"they'd\", \"i'd\", \"it'd\", \"they'll\", 'those', 'shouldn', 'theirs', 'd', 'for', \"it's\", \"haven't\", 'needn', 's', 'been', 'once', 'why', 'few', 'and', 'here', 'does', 'being', 'm', 'my', 'a', 'your', 'who', 'of', \"shouldn't\", 'any', 're', 'that', \"don't\", 'in', 'himself', 'will', \"he'd\", 'then', 'by', 'too', \"you're\", 'again', 'as', 'between', 'themselves', 'is', 'our', 'down', 'all', 'its', 'through', 'some', 'wouldn', \"wouldn't\", 'because', 'were', \"needn't\", \"we're\", 'couldn', 'each', 'weren', 'such', \"we've\", 'same', \"weren't\", 'above', 'they', 'until', 'very', 'herself', \"i'm\", 'are', 'her', 'there', 'ourselves', \"i'll\", 've', \"didn't\", \"we'd\"}\n",
      "Number of stop words: 198\n",
      "Text preprocessing complete.\n",
      "\n",
      "Document 1 Before preprocessing:\n",
      "A galaxy is a system of stars, stellar remnants, interstellar gas, dust, and dark matter bound together by gravity. The word is derived from the Greek galaxias (Î³Î±Î»Î±Î¾Î¯Î±Ï‚), literally 'milky', a reference to the Milky Way galaxy that contains the Solar System. Galaxies, averaging an estimated 100 million stars, range in size from dwarfs with less than a thousand stars, to the largest galaxies known â€“ supergiants with one hundred trillion stars, each orbiting its galaxy's centre of mass. Most of th\n",
      "\n",
      "Document 1 After preprocessing:\n",
      "galaxy system star stellar remnants interstellar gas dust dark matter bind together gravity word derive greek galaxias literally milky reference milky way galaxy contain solar system galaxies average estimate million star range size dwarf less thousand star largest galaxies know supergiants one hundred trillion star orbit galaxys centre mass mass typical galaxy form dark matter percent mass visible form star nebulae supermassive black hole common feature centre galaxies galaxies categorise accor\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Preprocess the Text ---\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"Stop words:\\n {stop_words}\")\n",
    "print(f\"Number of stop words: {len(stop_words)}\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "    # Remove stop words and lemmatize\n",
    "    processed_words = [lemmatizer.lemmatize(word, pos='v') for word in words if word not in stop_words]\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "processed_documents = [preprocess_text(doc) for doc in documents]\n",
    "print(\"Text preprocessing complete.\")\n",
    "\n",
    "print(f\"\\nDocument 1 Before preprocessing:\\n{documents[0][:500]}\")  # Print first 500 characters of the first original document\n",
    "print(f\"\\nDocument 1 After preprocessing:\\n{processed_documents[0][:500]}\")  # Print first 500 characters of the first processed document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03d26a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 5236 stored elements and shape (9, 1000)>\n",
      "  Coords\tValues\n",
      "  (0, 371)\t0.4036268253155702\n",
      "  (0, 898)\t0.011085748508054258\n",
      "  (0, 864)\t0.36822096344578337\n",
      "  (0, 870)\t0.031865275682808177\n",
      "  (0, 769)\t0.00625660127693623\n",
      "  (0, 463)\t0.034411307023149255\n",
      "  (0, 375)\t0.08021203859031145\n",
      "  (0, 268)\t0.037539607661617375\n",
      "  (0, 213)\t0.05005281021548984\n",
      "  (0, 540)\t0.045198205997058954\n",
      "  (0, 101)\t0.010026504823788931\n",
      "  (0, 927)\t0.013559461799117687\n",
      "  (0, 395)\t0.021243517121872117\n",
      "  (0, 991)\t0.0067797308995588435\n",
      "  (0, 561)\t0.17348872316195563\n",
      "  (0, 757)\t0.0036952495026847526\n",
      "  (0, 979)\t0.08577624081121084\n",
      "  (0, 190)\t0.024019121767450892\n",
      "  (0, 842)\t0.016748632904593976\n",
      "  (0, 370)\t0.6620896169650143\n",
      "  (0, 81)\t0.016338331583087777\n",
      "  (0, 307)\t0.01838062303097375\n",
      "  (0, 562)\t0.012933373259396634\n",
      "  (0, 733)\t0.007390499005369505\n",
      "  (0, 833)\t0.027118923598235374\n",
      "  :\t:\n",
      "  (8, 780)\t0.10684975235785324\n",
      "  (8, 841)\t0.19945287106799273\n",
      "  (8, 840)\t0.007123316823856883\n",
      "  (8, 4)\t0.04986321776699818\n",
      "  (8, 626)\t0.032248456995323575\n",
      "  (8, 91)\t0.007123316823856883\n",
      "  (8, 661)\t0.016124228497661788\n",
      "  (8, 481)\t0.2338013132160959\n",
      "  (8, 179)\t0.08062114248830894\n",
      "  (8, 166)\t0.008062114248830894\n",
      "  (8, 35)\t0.04031057124415447\n",
      "  (8, 78)\t0.016124228497661788\n",
      "  (8, 517)\t0.02418634274649268\n",
      "  (8, 176)\t0.008062114248830894\n",
      "  (8, 912)\t0.016124228497661788\n",
      "  (8, 559)\t0.02418634274649268\n",
      "  (8, 960)\t0.02418634274649268\n",
      "  (8, 393)\t0.008062114248830894\n",
      "  (8, 959)\t0.06449691399064715\n",
      "  (8, 472)\t0.032248456995323575\n",
      "  (8, 482)\t0.2982982272067431\n",
      "  (8, 406)\t0.05643479974181625\n",
      "  (8, 162)\t0.04837268549298536\n",
      "  (8, 185)\t0.008062114248830894\n",
      "  (8, 713)\t0.25035573510037074\n",
      "TF-IDF matrix created successfully.\n",
      "Shape of the matrix: (9, 1000)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Convert Text to Vectors ---\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000) # Limit to the top 1000 features\n",
    "\n",
    "# Create the TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_documents)\n",
    "\n",
    "print(tfidf_matrix)\n",
    "\n",
    "print(\"TF-IDF matrix created successfully.\")\n",
    "print(f\"Shape of the matrix: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b991cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Run K-Means ---\n",
    "\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=5)\n",
    "## The below line is functionally identical to the above line\n",
    "#kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Get the cluster assignments for each document\n",
    "labels = kmeans.labels_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9d9483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Evaluation Metrics ---\n",
      "WCSS (Inertia): 4.4805\n",
      "Silhouette Score: 0.1053\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Evaluation Model performance ---\n",
    "# --- 1. Calculate WCSS (Within-Cluster Sum of Squares) ---\n",
    "# This is already calculated automatically when you fit the model!\n",
    "# It is stored in the variable 'kmeans.inertia_'\n",
    "wcss = kmeans.inertia_\n",
    "\n",
    "# --- 2. Calculate Silhouette Score ---\n",
    "# This measures how well-separated the clusters are.\n",
    "# It takes the data (tfidf_matrix) and the labels the model assigned.\n",
    "sil_score = silhouette_score(tfidf_matrix, kmeans.labels_)\n",
    "\n",
    "print(\"--- Model Evaluation Metrics ---\")\n",
    "print(f\"WCSS (Inertia): {wcss:.4f}\")\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b917d486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: algorithm set welldefined instructions design perform specific task solve computational problem computer science study algorithms fundamental create efficient scalable software data structure array hash table use organize data way allow algorithms access manipulate effectively\n",
      "\n",
      "Shape of the new vector: (1, 1000)\n",
      "\n",
      "The new document belongs to cluster: 1\n"
     ]
    }
   ],
   "source": [
    "# --- Step 7: Putting the Model to Work - Predicting on New Documents ---\n",
    "# Now for the exciting part! We can take our final \"trained\" model and \n",
    "# use it to instantly categorize a brand new, unseen document. \n",
    "# Let's see which topic cluster it belongs to!\n",
    "\n",
    "# --- Define your new document ---\n",
    "new_text = \"An algorithm is a set of well-defined instructions designed to perform a specific task or solve a computational problem. In computer science, the study of algorithms is fundamental to creating efficient and scalable software. Data structures, such as arrays and hash tables, are used to organize data in a way that allows these algorithms to access and manipulate it effectively.\"\n",
    "\n",
    "# --- Apply the SAME preprocessing ---\n",
    "# We use the preprocess_text function we defined earlier\n",
    "processed_new_text = preprocess_text(new_text)\n",
    "print(f\"Cleaned Text: {processed_new_text}\")\n",
    "\n",
    "# --- Use the FITTED vectorizer to transform the text ---\n",
    "# IMPORTANT: Use .transform(), not .fit_transform()\n",
    "# This ensures it uses the same vocabulary learned from the original documents.\n",
    "new_tfidf_vector = vectorizer.transform([processed_new_text])\n",
    "\n",
    "print(f\"\\nShape of the new vector: {new_tfidf_vector.shape}\")\n",
    "\n",
    "# --- Now you can predict its cluster ---\n",
    "predicted_label = kmeans.predict(new_tfidf_vector)\n",
    "\n",
    "print(f\"\\nThe new document belongs to cluster: {predicted_label[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
